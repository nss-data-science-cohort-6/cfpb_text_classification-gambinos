{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.svm import LinearSVC\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[37], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39m# Load the entire .pkl file into a DataFrame\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m complaints \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39;49mread_pickle(\u001b[39m'\u001b[39;49m\u001b[39m../data/complaints_nlp.pkl\u001b[39;49m\u001b[39m'\u001b[39;49m)\n\u001b[1;32m      4\u001b[0m \u001b[39m# Select features\u001b[39;00m\n\u001b[1;32m      5\u001b[0m columns_to_import \u001b[39m=\u001b[39m [\u001b[39m'\u001b[39m\u001b[39missue\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mtokens\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mstems\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mlemmas\u001b[39m\u001b[39m'\u001b[39m]\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.9/site-packages/pandas/io/pickle.py:208\u001b[0m, in \u001b[0;36mread_pickle\u001b[0;34m(filepath_or_buffer, compression, storage_options)\u001b[0m\n\u001b[1;32m    205\u001b[0m     \u001b[39mwith\u001b[39;00m warnings\u001b[39m.\u001b[39mcatch_warnings(record\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m):\n\u001b[1;32m    206\u001b[0m         \u001b[39m# We want to silence any warnings about, e.g. moved modules.\u001b[39;00m\n\u001b[1;32m    207\u001b[0m         warnings\u001b[39m.\u001b[39msimplefilter(\u001b[39m\"\u001b[39m\u001b[39mignore\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mWarning\u001b[39;00m)\n\u001b[0;32m--> 208\u001b[0m         \u001b[39mreturn\u001b[39;00m pickle\u001b[39m.\u001b[39;49mload(handles\u001b[39m.\u001b[39;49mhandle)\n\u001b[1;32m    209\u001b[0m \u001b[39mexcept\u001b[39;00m excs_to_catch:\n\u001b[1;32m    210\u001b[0m     \u001b[39m# e.g.\u001b[39;00m\n\u001b[1;32m    211\u001b[0m     \u001b[39m#  \"No module named 'pandas.core.sparse.series'\"\u001b[39;00m\n\u001b[1;32m    212\u001b[0m     \u001b[39m#  \"Can't get attribute '__nat_unpickle' on <module 'pandas._libs.tslib\"\u001b[39;00m\n\u001b[1;32m    213\u001b[0m     \u001b[39mreturn\u001b[39;00m pc\u001b[39m.\u001b[39mload(handles\u001b[39m.\u001b[39mhandle, encoding\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.9/site-packages/pandas/_libs/internals.pyx:575\u001b[0m, in \u001b[0;36mpandas._libs.internals._unpickle_block\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.9/site-packages/pandas/core/internals/blocks.py:2172\u001b[0m, in \u001b[0;36mnew_block\u001b[0;34m(values, placement, ndim)\u001b[0m\n\u001b[1;32m   2168\u001b[0m     values \u001b[39m=\u001b[39m maybe_coerce_values(values)\n\u001b[1;32m   2169\u001b[0m     \u001b[39mreturn\u001b[39;00m klass(values, ndim\u001b[39m=\u001b[39m\u001b[39m2\u001b[39m, placement\u001b[39m=\u001b[39mplacement)\n\u001b[0;32m-> 2172\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mnew_block\u001b[39m(values, placement, \u001b[39m*\u001b[39m, ndim: \u001b[39mint\u001b[39m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Block:\n\u001b[1;32m   2173\u001b[0m     \u001b[39m# caller is responsible for ensuring values is NOT a PandasArray\u001b[39;00m\n\u001b[1;32m   2175\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39misinstance\u001b[39m(placement, BlockPlacement):\n\u001b[1;32m   2176\u001b[0m         placement \u001b[39m=\u001b[39m BlockPlacement(placement)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Load the entire .pkl file into a DataFrame\n",
    "complaints = pd.read_pickle('../data/complaints_nlp.pkl')\n",
    "\n",
    "# Select features\n",
    "columns_to_import = ['issue', 'tokens', 'stems', 'lemmas']\n",
    "complaints = complaints[columns_to_import]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bag of Words Analysis"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----------------\n",
    "### BOW: Tokens \n",
    "> Not Fruitful.  Focus on Lemmatised and Stemmed.  **Should I even be doing this on the tokens!?!?!?** I expect that there is an issue with the size of the matrices that I'm working with.  Perhaps focusing on dimension reduction is where I need to go.\n",
    "_____________________"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### BOW Tokens: - Count Vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an instance of CountVectorizer for tokens\n",
    "\n",
    "vectorizer_tokens = CountVectorizer(min_df=2)\n",
    "X_tokens = vectorizer_tokens.fit_transform(complaints['tokens'].apply(lambda x: ' '.join(x)))\n",
    "\n",
    "# Create a DataFrame from the sparse matrix\n",
    "\n",
    "bag_of_words_tokens_df = pd.DataFrame.sparse.from_spmatrix(X_tokens, columns=vectorizer_tokens.get_feature_names_out())\n",
    "\n",
    "# Concatenate the bag-of-words DataFrame with the 'issue' column\n",
    "\n",
    "bag_of_words_tokens_df = pd.concat([complaints['issue'], bag_of_words_tokens_df], axis=1)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### BOW Tokens: Train/Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                     \r"
     ]
    }
   ],
   "source": [
    "X = bag_of_words_tokens_df.drop('issue', axis=1)\n",
    "y = bag_of_words_tokens_df['issue']\n",
    "X_train, X_test, y_train, y_test = tqdm(train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42), total=1, leave=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### BOW Tokens: Naive Bayes Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chunderdamus/anaconda3/lib/python3.9/site-packages/sklearn/utils/validation.py:768: UserWarning: pandas.DataFrame with sparse columns found.It will be converted to a dense numpy array.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "y should be a 1d array, got an array of shape (282745, 2) instead.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[31], line 5\u001b[0m\n\u001b[1;32m      2\u001b[0m nb_model \u001b[39m=\u001b[39m MultinomialNB()\n\u001b[1;32m      4\u001b[0m \u001b[39m#Train The the model\u001b[39;00m\n\u001b[0;32m----> 5\u001b[0m nb_model\u001b[39m.\u001b[39;49mfit(X_train, y_train)\n\u001b[1;32m      7\u001b[0m \u001b[39m# Make Predictions from model\u001b[39;00m\n\u001b[1;32m      8\u001b[0m y_pred \u001b[39m=\u001b[39m nb_model\u001b[39m.\u001b[39mpredict(X_test)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.9/site-packages/sklearn/naive_bayes.py:749\u001b[0m, in \u001b[0;36m_BaseDiscreteNB.fit\u001b[0;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[1;32m    729\u001b[0m \u001b[39m\"\"\"Fit Naive Bayes classifier according to X, y.\u001b[39;00m\n\u001b[1;32m    730\u001b[0m \n\u001b[1;32m    731\u001b[0m \u001b[39mParameters\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    746\u001b[0m \u001b[39m    Returns the instance itself.\u001b[39;00m\n\u001b[1;32m    747\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    748\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_validate_params()\n\u001b[0;32m--> 749\u001b[0m X, y \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_check_X_y(X, y)\n\u001b[1;32m    750\u001b[0m _, n_features \u001b[39m=\u001b[39m X\u001b[39m.\u001b[39mshape\n\u001b[1;32m    752\u001b[0m labelbin \u001b[39m=\u001b[39m LabelBinarizer()\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.9/site-packages/sklearn/naive_bayes.py:583\u001b[0m, in \u001b[0;36m_BaseDiscreteNB._check_X_y\u001b[0;34m(self, X, y, reset)\u001b[0m\n\u001b[1;32m    581\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_check_X_y\u001b[39m(\u001b[39mself\u001b[39m, X, y, reset\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m):\n\u001b[1;32m    582\u001b[0m     \u001b[39m\"\"\"Validate X and y in fit methods.\"\"\"\u001b[39;00m\n\u001b[0;32m--> 583\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_validate_data(X, y, accept_sparse\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mcsr\u001b[39;49m\u001b[39m\"\u001b[39;49m, reset\u001b[39m=\u001b[39;49mreset)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.9/site-packages/sklearn/base.py:584\u001b[0m, in \u001b[0;36mBaseEstimator._validate_data\u001b[0;34m(self, X, y, reset, validate_separately, **check_params)\u001b[0m\n\u001b[1;32m    582\u001b[0m         y \u001b[39m=\u001b[39m check_array(y, input_name\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39my\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mcheck_y_params)\n\u001b[1;32m    583\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 584\u001b[0m         X, y \u001b[39m=\u001b[39m check_X_y(X, y, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mcheck_params)\n\u001b[1;32m    585\u001b[0m     out \u001b[39m=\u001b[39m X, y\n\u001b[1;32m    587\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m no_val_X \u001b[39mand\u001b[39;00m check_params\u001b[39m.\u001b[39mget(\u001b[39m\"\u001b[39m\u001b[39mensure_2d\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mTrue\u001b[39;00m):\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.9/site-packages/sklearn/utils/validation.py:1122\u001b[0m, in \u001b[0;36mcheck_X_y\u001b[0;34m(X, y, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, multi_output, ensure_min_samples, ensure_min_features, y_numeric, estimator)\u001b[0m\n\u001b[1;32m   1102\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m   1103\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00mestimator_name\u001b[39m}\u001b[39;00m\u001b[39m requires y to be passed, but the target y is None\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   1104\u001b[0m     )\n\u001b[1;32m   1106\u001b[0m X \u001b[39m=\u001b[39m check_array(\n\u001b[1;32m   1107\u001b[0m     X,\n\u001b[1;32m   1108\u001b[0m     accept_sparse\u001b[39m=\u001b[39maccept_sparse,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1119\u001b[0m     input_name\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mX\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m   1120\u001b[0m )\n\u001b[0;32m-> 1122\u001b[0m y \u001b[39m=\u001b[39m _check_y(y, multi_output\u001b[39m=\u001b[39;49mmulti_output, y_numeric\u001b[39m=\u001b[39;49my_numeric, estimator\u001b[39m=\u001b[39;49mestimator)\n\u001b[1;32m   1124\u001b[0m check_consistent_length(X, y)\n\u001b[1;32m   1126\u001b[0m \u001b[39mreturn\u001b[39;00m X, y\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.9/site-packages/sklearn/utils/validation.py:1143\u001b[0m, in \u001b[0;36m_check_y\u001b[0;34m(y, multi_output, y_numeric, estimator)\u001b[0m\n\u001b[1;32m   1141\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m   1142\u001b[0m     estimator_name \u001b[39m=\u001b[39m _check_estimator_name(estimator)\n\u001b[0;32m-> 1143\u001b[0m     y \u001b[39m=\u001b[39m column_or_1d(y, warn\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n\u001b[1;32m   1144\u001b[0m     _assert_all_finite(y, input_name\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39my\u001b[39m\u001b[39m\"\u001b[39m, estimator_name\u001b[39m=\u001b[39mestimator_name)\n\u001b[1;32m   1145\u001b[0m     _ensure_no_complex_data(y)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.9/site-packages/sklearn/utils/validation.py:1202\u001b[0m, in \u001b[0;36mcolumn_or_1d\u001b[0;34m(y, dtype, warn)\u001b[0m\n\u001b[1;32m   1193\u001b[0m         warnings\u001b[39m.\u001b[39mwarn(\n\u001b[1;32m   1194\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mA column-vector y was passed when a 1d array was\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   1195\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39m expected. Please change the shape of y to \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1198\u001b[0m             stacklevel\u001b[39m=\u001b[39m\u001b[39m2\u001b[39m,\n\u001b[1;32m   1199\u001b[0m         )\n\u001b[1;32m   1200\u001b[0m     \u001b[39mreturn\u001b[39;00m _asarray_with_order(xp\u001b[39m.\u001b[39mreshape(y, \u001b[39m-\u001b[39m\u001b[39m1\u001b[39m), order\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mC\u001b[39m\u001b[39m\"\u001b[39m, xp\u001b[39m=\u001b[39mxp)\n\u001b[0;32m-> 1202\u001b[0m \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m   1203\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39my should be a 1d array, got an array of shape \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m instead.\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(shape)\n\u001b[1;32m   1204\u001b[0m )\n",
      "\u001b[0;31mValueError\u001b[0m: y should be a 1d array, got an array of shape (282745, 2) instead."
     ]
    }
   ],
   "source": [
    "# Create an instance of the Naive Bayes model\n",
    "nb_model = MultinomialNB()\n",
    "\n",
    "#Train The the model\n",
    "nb_model.fit(X_train, y_train)\n",
    "\n",
    "# Make Predictions from model\n",
    "y_pred = nb_model.predict(X_test)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It keeps yelling at me for having a sparse matrix when fitting the model . . . When I try to go dense, It crashes the kernel.  I even tried to chunk the proccess, but no joy.  I thought I needed a sparse matrix.  Dense is several gb in allocation.  If I had hair, I'd pull it :-("
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---------------\n",
    "### BOW Stemms\n",
    "_____________"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### BOW Stemms: CountVectorizer Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bag-of-Words Stems:\n",
      "   issue  aa  aaa  aabout  aaccount  aacord  aacount  aadvantag  aaf  aag  \\\n",
      "0      4   0    0       0         0       0        0          0    0    0   \n",
      "1      3   0    0       0         0       0        0          0    0    0   \n",
      "2      4   0    0       0         0       0        0          0    0    0   \n",
      "3      1   0    0       0         0       0        0          0    0    0   \n",
      "4      4   0    0       0         0       0        0          0    0    0   \n",
      "\n",
      "   ...  zinc  zion  zip  zipcod  zombi  zone  zoom  zt  zuntafi  zwicker  \n",
      "0  ...     0     0    0       0      0     0     0   0        0        0  \n",
      "1  ...     0     0    0       0      0     0     0   0        0        0  \n",
      "2  ...     0     0    0       0      0     0     0   0        0        0  \n",
      "3  ...     0     0    0       0      0     0     0   0        0        0  \n",
      "4  ...     0     0    0       0      0     0     0   0        0        0  \n",
      "\n",
      "[5 rows x 26067 columns]\n"
     ]
    }
   ],
   "source": [
    "# Create an instance of CountVectorizer for stems\n",
    "vectorizer_stems = CountVectorizer(min_df=3)\n",
    "X_stems = vectorizer_stems.fit_transform(complaints['stems'].apply(lambda x: ' '.join(x)))\n",
    "\n",
    "# Create a DataFrame from the sparse matrix\n",
    "bow_stems = pd.DataFrame.sparse.from_spmatrix(X_stems, columns=vectorizer_stems.get_feature_names_out())\n",
    "\n",
    "# Concatenate the bag-of-words DataFrame with the 'issue' column\n",
    "bow_stems = pd.concat([complaints['issue'], bow_stems], axis=1)\n",
    "\n",
    "print(\"Bag-of-Words Stems:\")\n",
    "print(bow_stems.head())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With no limit placed on word frequency, the stems vectorizer produces a matrix of **75,905** dimensions.  \n",
    "- With a minimum word occurance of 2, dimensions were reduced to **36,811**.  This means: \n",
    "    - There are **39,094** word stems which occur only once.\n",
    "    - Single occurrance stems represent **51.50%** of the dimensions that occur in the matrix.\n",
    "    - **This may also indicate that tokens derived from the corpus require additional processing before stemming so that more words may be salvaged.** \n",
    "- With a minimum word occurrance of 3, the dimensions were reduced to **26,067**.  This means that\n",
    "    - There was a **29.19%** dimension reduction from `min_df=2`.\n",
    "    - **34.34%** of stems lemmas occur more than three times.\n",
    "    - **65.66%** of stems occur 3 or fewer times."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### BOW Stems: TF-IDF Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TF-IDF Stems:\n",
      "   issue   aa  aaa  aaaaan  aaac  aaadvantag  aaaf  aaall  aaargon  aab  ...  \\\n",
      "0      4  0.0  0.0     0.0   0.0         0.0   0.0    0.0      0.0  0.0  ...   \n",
      "1      3  0.0  0.0     0.0   0.0         0.0   0.0    0.0      0.0  0.0  ...   \n",
      "2      4  0.0  0.0     0.0   0.0         0.0   0.0    0.0      0.0  0.0  ...   \n",
      "3      1  0.0  0.0     0.0   0.0         0.0   0.0    0.0      0.0  0.0  ...   \n",
      "4      4  0.0  0.0     0.0   0.0         0.0   0.0    0.0      0.0  0.0  ...   \n",
      "\n",
      "   zoom  zoombi  zoomer  zoomsup   zt  zuntafi  zuri  zwick  zwicker  zzzz  \n",
      "0   0.0     0.0     0.0      0.0  0.0      0.0   0.0    0.0      0.0   0.0  \n",
      "1   0.0     0.0     0.0      0.0  0.0      0.0   0.0    0.0      0.0   0.0  \n",
      "2   0.0     0.0     0.0      0.0  0.0      0.0   0.0    0.0      0.0   0.0  \n",
      "3   0.0     0.0     0.0      0.0  0.0      0.0   0.0    0.0      0.0   0.0  \n",
      "4   0.0     0.0     0.0      0.0  0.0      0.0   0.0    0.0      0.0   0.0  \n",
      "\n",
      "[5 rows x 75905 columns]\n"
     ]
    }
   ],
   "source": [
    "# Create an instance of TfidfVectorizer:\n",
    "tfidf = TfidfVectorizer(min_df=1)\n",
    "\n",
    "# Fit and transform the stemmed text data using TfidfVectorizer:\n",
    "X_stems = tfidf.fit_transform(complaints['stems'].apply(lambda x: ' '.join(x)))\n",
    "\n",
    "# Create a DataFrame from the sparse matrix:\n",
    "tfidf_stems = pd.DataFrame.sparse.from_spmatrix(X_stems, columns=tfidf.get_feature_names_out())\n",
    "\n",
    "# Concatenate the TF-IDF DataFrame with the 'issue' column:\n",
    "tfidf_stems = pd.concat([complaints['issue'], tfidf_stems], axis=1)\n",
    "\n",
    "# Print the TF-IDF representation:\n",
    "print(\"TF-IDF Stems:\")\n",
    "print(tfidf_stems.head())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Applying TF-IDF to the stemms seems to have no effect on reducing dimensionality.  Again, do I need more preprocessing, or am I missing something else along the way?"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### BOW Tokens - Train/Test Split (include later if neccessary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--------------\n",
    "### BOW Lemmas\n",
    "____________"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### BOW Lemmas: CountVectorizer Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bag-of-Words Lemmas - CountVectorizer:\n",
      "   issue  aa  aaa  aabout  aaccount  aacount  aadvantage  aafes  aag  aai  \\\n",
      "0      4   0    0       0         0        0           0      0    0    0   \n",
      "1      3   0    0       0         0        0           0      0    0    0   \n",
      "2      4   0    0       0         0        0           0      0    0    0   \n",
      "3      1   0    0       0         0        0           0      0    0    0   \n",
      "4      4   0    0       0         0        0           0      0    0    0   \n",
      "\n",
      "   ...  zip  zipcode  zombie  zone  zoned  zoning  zoom  zt  zuntafi  zwicker  \n",
      "0  ...    0        0       0     0      0       0     0   0        0        0  \n",
      "1  ...    0        0       0     0      0       0     0   0        0        0  \n",
      "2  ...    0        0       0     0      0       0     0   0        0        0  \n",
      "3  ...    0        0       0     0      0       0     0   0        0        0  \n",
      "4  ...    0        0       0     0      0       0     0   0        0        0  \n",
      "\n",
      "[5 rows x 34220 columns]\n"
     ]
    }
   ],
   "source": [
    "# Create an instance of CountVectorizer for lemmas\n",
    "vectorizer_lemmas = CountVectorizer(min_df=3)\n",
    "X_lemmas = vectorizer_lemmas.fit_transform(complaints['lemmas'].apply(lambda x: ' '.join(x)))\n",
    "\n",
    "# Create a DataFrame from the sparse matrix\n",
    "bow_lemmas_cv = pd.DataFrame.sparse.from_spmatrix(X_lemmas, columns=vectorizer_lemmas.get_feature_names_out())\n",
    "\n",
    "# Concatenate the bag-of-words DataFrame with the 'issue' column\n",
    "bow_lemmas_cv = pd.concat([complaints['issue'], bow_lemmas_cv], axis=1)\n",
    "\n",
    "print(\"Bag-of-Words Lemmas - CountVectorizer:\")\n",
    "print(bow_lemmas_cv.head())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With no limit placed on word frequency, the **lemmas CountVectorizer** produces a matrix of **94,020** dimensions.  This means that the `lemma` matrix has **18,115** more dimensions than the `stemms` matrix in its current state.  \n",
    "- With a minimum word occurance of 2, dimensions were reduced to **47,258**.  This means: \n",
    "    - There are **46,762** lemmas which occur only once.\n",
    "    - Single occurrance lemmas represent **50.26%** of the dimensions that occur in the matrix.\n",
    "    - **This may also indicate that tokens derived from the corpus require additional processing before lematization so that more words may be salvaged.** \n",
    "- With a minimum word occurrance of 3, the dimensions were reduced to **34,220**.  This means that:\n",
    "    - There was a **27.59%** dimension reduction from `min_df=2`.\n",
    "    - **36.40%** of word stems occur 3 or more times.\n",
    "    - **63.60%** of the stems occur fewer than 3 times."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### BOW Lemmas: TF-IDF Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TF-IDF Lemmas:\n",
      "   issue   aa  aaa  aaaf  aabout  aac  aacbna  aaccording  aaccount  \\\n",
      "0      4  0.0  0.0   0.0     0.0  0.0     0.0         0.0       0.0   \n",
      "1      3  0.0  0.0   0.0     0.0  0.0     0.0         0.0       0.0   \n",
      "2      4  0.0  0.0   0.0     0.0  0.0     0.0         0.0       0.0   \n",
      "3      1  0.0  0.0   0.0     0.0  0.0     0.0         0.0       0.0   \n",
      "4      4  0.0  0.0   0.0     0.0  0.0     0.0         0.0       0.0   \n",
      "\n",
      "   aacording  ...  zombie  zone  zoned  zoning  zoom  zoomed  zooming   zt  \\\n",
      "0        0.0  ...     0.0   0.0    0.0     0.0   0.0     0.0      0.0  0.0   \n",
      "1        0.0  ...     0.0   0.0    0.0     0.0   0.0     0.0      0.0  0.0   \n",
      "2        0.0  ...     0.0   0.0    0.0     0.0   0.0     0.0      0.0  0.0   \n",
      "3        0.0  ...     0.0   0.0    0.0     0.0   0.0     0.0      0.0  0.0   \n",
      "4        0.0  ...     0.0   0.0    0.0     0.0   0.0     0.0      0.0  0.0   \n",
      "\n",
      "   zuntafi  zwicker  \n",
      "0      0.0      0.0  \n",
      "1      0.0      0.0  \n",
      "2      0.0      0.0  \n",
      "3      0.0      0.0  \n",
      "4      0.0      0.0  \n",
      "\n",
      "[5 rows x 47258 columns]\n"
     ]
    }
   ],
   "source": [
    "# Set Min Word occurance\n",
    "occurance = 2\n",
    "\n",
    "# Create an instance of TfidfVectorizer:\n",
    "tfidf = TfidfVectorizer(min_df = occurance)\n",
    "\n",
    "# Fit and transform the stemmed text data using TfidfVectorizer:\n",
    "X_lemmas = tfidf.fit_transform(complaints['lemmas'].apply(lambda x: ' '.join(x)))\n",
    "\n",
    "# Create a DataFrame from the sparse matrix:\n",
    "tfidf_lemmas = pd.DataFrame.sparse.from_spmatrix(X_lemmas, columns=tfidf.get_feature_names_out())\n",
    "\n",
    "# Concatenate the TF-IDF DataFrame with the 'issue' column:\n",
    "tfidf_lemmas = pd.concat([complaints['issue'], tfidf_lemmas], axis=1)\n",
    "\n",
    "# Print the TF-IDF representation:\n",
    "print(\"TF-IDF Lemmas:\")\n",
    "print(tfidf_lemmas.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[40], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m X \u001b[39m=\u001b[39m tfidf_lemmas\u001b[39m.\u001b[39mdrop(\u001b[39m'\u001b[39m\u001b[39missue\u001b[39m\u001b[39m'\u001b[39m, axis\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m)  \u001b[39m# Features (TF-IDF vectors)\u001b[39;00m\n\u001b[1;32m      2\u001b[0m y \u001b[39m=\u001b[39m tfidf_lemmas[\u001b[39m'\u001b[39m\u001b[39missue\u001b[39m\u001b[39m'\u001b[39m]  \u001b[39m# Target variable (issue category)\u001b[39;00m\n\u001b[0;32m----> 4\u001b[0m X_train, X_test, y_train, y_test \u001b[39m=\u001b[39m train_test_split(X, y, test_size\u001b[39m=\u001b[39;49m\u001b[39m0.2\u001b[39;49m, random_state\u001b[39m=\u001b[39;49m\u001b[39m42\u001b[39;49m)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.9/site-packages/sklearn/model_selection/_split.py:2585\u001b[0m, in \u001b[0;36mtrain_test_split\u001b[0;34m(test_size, train_size, random_state, shuffle, stratify, *arrays)\u001b[0m\n\u001b[1;32m   2581\u001b[0m     cv \u001b[39m=\u001b[39m CVClass(test_size\u001b[39m=\u001b[39mn_test, train_size\u001b[39m=\u001b[39mn_train, random_state\u001b[39m=\u001b[39mrandom_state)\n\u001b[1;32m   2583\u001b[0m     train, test \u001b[39m=\u001b[39m \u001b[39mnext\u001b[39m(cv\u001b[39m.\u001b[39msplit(X\u001b[39m=\u001b[39marrays[\u001b[39m0\u001b[39m], y\u001b[39m=\u001b[39mstratify))\n\u001b[0;32m-> 2585\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mlist\u001b[39;49m(\n\u001b[1;32m   2586\u001b[0m     chain\u001b[39m.\u001b[39;49mfrom_iterable(\n\u001b[1;32m   2587\u001b[0m         (_safe_indexing(a, train), _safe_indexing(a, test)) \u001b[39mfor\u001b[39;49;00m a \u001b[39min\u001b[39;49;00m arrays\n\u001b[1;32m   2588\u001b[0m     )\n\u001b[1;32m   2589\u001b[0m )\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.9/site-packages/sklearn/model_selection/_split.py:2587\u001b[0m, in \u001b[0;36m<genexpr>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m   2581\u001b[0m     cv \u001b[39m=\u001b[39m CVClass(test_size\u001b[39m=\u001b[39mn_test, train_size\u001b[39m=\u001b[39mn_train, random_state\u001b[39m=\u001b[39mrandom_state)\n\u001b[1;32m   2583\u001b[0m     train, test \u001b[39m=\u001b[39m \u001b[39mnext\u001b[39m(cv\u001b[39m.\u001b[39msplit(X\u001b[39m=\u001b[39marrays[\u001b[39m0\u001b[39m], y\u001b[39m=\u001b[39mstratify))\n\u001b[1;32m   2585\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mlist\u001b[39m(\n\u001b[1;32m   2586\u001b[0m     chain\u001b[39m.\u001b[39mfrom_iterable(\n\u001b[0;32m-> 2587\u001b[0m         (_safe_indexing(a, train), _safe_indexing(a, test)) \u001b[39mfor\u001b[39;00m a \u001b[39min\u001b[39;00m arrays\n\u001b[1;32m   2588\u001b[0m     )\n\u001b[1;32m   2589\u001b[0m )\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.9/site-packages/sklearn/utils/__init__.py:354\u001b[0m, in \u001b[0;36m_safe_indexing\u001b[0;34m(X, indices, axis)\u001b[0m\n\u001b[1;32m    348\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m    349\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mSpecifying the columns using strings is only supported for \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    350\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mpandas DataFrames\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    351\u001b[0m     )\n\u001b[1;32m    353\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mhasattr\u001b[39m(X, \u001b[39m\"\u001b[39m\u001b[39miloc\u001b[39m\u001b[39m\"\u001b[39m):\n\u001b[0;32m--> 354\u001b[0m     \u001b[39mreturn\u001b[39;00m _pandas_indexing(X, indices, indices_dtype, axis\u001b[39m=\u001b[39;49maxis)\n\u001b[1;32m    355\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mhasattr\u001b[39m(X, \u001b[39m\"\u001b[39m\u001b[39mshape\u001b[39m\u001b[39m\"\u001b[39m):\n\u001b[1;32m    356\u001b[0m     \u001b[39mreturn\u001b[39;00m _array_indexing(X, indices, indices_dtype, axis\u001b[39m=\u001b[39maxis)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.9/site-packages/sklearn/utils/__init__.py:196\u001b[0m, in \u001b[0;36m_pandas_indexing\u001b[0;34m(X, key, key_dtype, axis)\u001b[0m\n\u001b[1;32m    191\u001b[0m     key \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39masarray(key)\n\u001b[1;32m    193\u001b[0m \u001b[39mif\u001b[39;00m key_dtype \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mint\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39misinstance\u001b[39m(key, \u001b[39mslice\u001b[39m) \u001b[39mor\u001b[39;00m np\u001b[39m.\u001b[39misscalar(key)):\n\u001b[1;32m    194\u001b[0m     \u001b[39m# using take() instead of iloc[] ensures the return value is a \"proper\"\u001b[39;00m\n\u001b[1;32m    195\u001b[0m     \u001b[39m# copy that will not raise SettingWithCopyWarning\u001b[39;00m\n\u001b[0;32m--> 196\u001b[0m     \u001b[39mreturn\u001b[39;00m X\u001b[39m.\u001b[39;49mtake(key, axis\u001b[39m=\u001b[39;49maxis)\n\u001b[1;32m    197\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    198\u001b[0m     \u001b[39m# check whether we should index with loc or iloc\u001b[39;00m\n\u001b[1;32m    199\u001b[0m     indexer \u001b[39m=\u001b[39m X\u001b[39m.\u001b[39miloc \u001b[39mif\u001b[39;00m key_dtype \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mint\u001b[39m\u001b[39m\"\u001b[39m \u001b[39melse\u001b[39;00m X\u001b[39m.\u001b[39mloc\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.9/site-packages/pandas/core/generic.py:3871\u001b[0m, in \u001b[0;36mNDFrame.take\u001b[0;34m(self, indices, axis, is_copy, **kwargs)\u001b[0m\n\u001b[1;32m   3862\u001b[0m     warnings\u001b[39m.\u001b[39mwarn(\n\u001b[1;32m   3863\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mis_copy is deprecated and will be removed in a future version. \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   3864\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39m'\u001b[39m\u001b[39mtake\u001b[39m\u001b[39m'\u001b[39m\u001b[39m always returns a copy, so there is no need to specify this.\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m   3865\u001b[0m         \u001b[39mFutureWarning\u001b[39;00m,\n\u001b[1;32m   3866\u001b[0m         stacklevel\u001b[39m=\u001b[39mfind_stack_level(),\n\u001b[1;32m   3867\u001b[0m     )\n\u001b[1;32m   3869\u001b[0m nv\u001b[39m.\u001b[39mvalidate_take((), kwargs)\n\u001b[0;32m-> 3871\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_take(indices, axis)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.9/site-packages/pandas/core/generic.py:3886\u001b[0m, in \u001b[0;36mNDFrame._take\u001b[0;34m(self, indices, axis, convert_indices)\u001b[0m\n\u001b[1;32m   3879\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m   3880\u001b[0m \u001b[39mInternal version of the `take` allowing specification of additional args.\u001b[39;00m\n\u001b[1;32m   3881\u001b[0m \n\u001b[1;32m   3882\u001b[0m \u001b[39mSee the docstring of `take` for full explanation of the parameters.\u001b[39;00m\n\u001b[1;32m   3883\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m   3884\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_consolidate_inplace()\n\u001b[0;32m-> 3886\u001b[0m new_data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_mgr\u001b[39m.\u001b[39;49mtake(\n\u001b[1;32m   3887\u001b[0m     indices,\n\u001b[1;32m   3888\u001b[0m     axis\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_get_block_manager_axis(axis),\n\u001b[1;32m   3889\u001b[0m     verify\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,\n\u001b[1;32m   3890\u001b[0m     convert_indices\u001b[39m=\u001b[39;49mconvert_indices,\n\u001b[1;32m   3891\u001b[0m )\n\u001b[1;32m   3892\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_constructor(new_data)\u001b[39m.\u001b[39m__finalize__(\u001b[39mself\u001b[39m, method\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mtake\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.9/site-packages/pandas/core/internals/managers.py:978\u001b[0m, in \u001b[0;36mBaseBlockManager.take\u001b[0;34m(self, indexer, axis, verify, convert_indices)\u001b[0m\n\u001b[1;32m    975\u001b[0m     indexer \u001b[39m=\u001b[39m maybe_convert_indices(indexer, n, verify\u001b[39m=\u001b[39mverify)\n\u001b[1;32m    977\u001b[0m new_labels \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39maxes[axis]\u001b[39m.\u001b[39mtake(indexer)\n\u001b[0;32m--> 978\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mreindex_indexer(\n\u001b[1;32m    979\u001b[0m     new_axis\u001b[39m=\u001b[39;49mnew_labels,\n\u001b[1;32m    980\u001b[0m     indexer\u001b[39m=\u001b[39;49mindexer,\n\u001b[1;32m    981\u001b[0m     axis\u001b[39m=\u001b[39;49maxis,\n\u001b[1;32m    982\u001b[0m     allow_dups\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,\n\u001b[1;32m    983\u001b[0m     copy\u001b[39m=\u001b[39;49m\u001b[39mNone\u001b[39;49;00m,\n\u001b[1;32m    984\u001b[0m )\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.9/site-packages/pandas/core/internals/managers.py:751\u001b[0m, in \u001b[0;36mBaseBlockManager.reindex_indexer\u001b[0;34m(self, new_axis, indexer, axis, fill_value, allow_dups, copy, only_slice, use_na_proxy)\u001b[0m\n\u001b[1;32m    749\u001b[0m     parent \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m \u001b[39mif\u001b[39;00m com\u001b[39m.\u001b[39mall_none(\u001b[39m*\u001b[39mnew_refs) \u001b[39melse\u001b[39;00m \u001b[39mself\u001b[39m\n\u001b[1;32m    750\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 751\u001b[0m     new_blocks \u001b[39m=\u001b[39m [\n\u001b[1;32m    752\u001b[0m         blk\u001b[39m.\u001b[39mtake_nd(\n\u001b[1;32m    753\u001b[0m             indexer,\n\u001b[1;32m    754\u001b[0m             axis\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m,\n\u001b[1;32m    755\u001b[0m             fill_value\u001b[39m=\u001b[39m(\n\u001b[1;32m    756\u001b[0m                 fill_value \u001b[39mif\u001b[39;00m fill_value \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m blk\u001b[39m.\u001b[39mfill_value\n\u001b[1;32m    757\u001b[0m             ),\n\u001b[1;32m    758\u001b[0m         )\n\u001b[1;32m    759\u001b[0m         \u001b[39mfor\u001b[39;00m blk \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mblocks\n\u001b[1;32m    760\u001b[0m     ]\n\u001b[1;32m    761\u001b[0m     new_refs \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m    762\u001b[0m     parent \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.9/site-packages/pandas/core/internals/managers.py:752\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    749\u001b[0m     parent \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m \u001b[39mif\u001b[39;00m com\u001b[39m.\u001b[39mall_none(\u001b[39m*\u001b[39mnew_refs) \u001b[39melse\u001b[39;00m \u001b[39mself\u001b[39m\n\u001b[1;32m    750\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    751\u001b[0m     new_blocks \u001b[39m=\u001b[39m [\n\u001b[0;32m--> 752\u001b[0m         blk\u001b[39m.\u001b[39;49mtake_nd(\n\u001b[1;32m    753\u001b[0m             indexer,\n\u001b[1;32m    754\u001b[0m             axis\u001b[39m=\u001b[39;49m\u001b[39m1\u001b[39;49m,\n\u001b[1;32m    755\u001b[0m             fill_value\u001b[39m=\u001b[39;49m(\n\u001b[1;32m    756\u001b[0m                 fill_value \u001b[39mif\u001b[39;49;00m fill_value \u001b[39mis\u001b[39;49;00m \u001b[39mnot\u001b[39;49;00m \u001b[39mNone\u001b[39;49;00m \u001b[39melse\u001b[39;49;00m blk\u001b[39m.\u001b[39;49mfill_value\n\u001b[1;32m    757\u001b[0m             ),\n\u001b[1;32m    758\u001b[0m         )\n\u001b[1;32m    759\u001b[0m         \u001b[39mfor\u001b[39;00m blk \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mblocks\n\u001b[1;32m    760\u001b[0m     ]\n\u001b[1;32m    761\u001b[0m     new_refs \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m    762\u001b[0m     parent \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.9/site-packages/pandas/core/internals/blocks.py:1775\u001b[0m, in \u001b[0;36mExtensionBlock.take_nd\u001b[0;34m(self, indexer, axis, new_mgr_locs, fill_value)\u001b[0m\n\u001b[1;32m   1769\u001b[0m     fill_value \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m   1771\u001b[0m \u001b[39m# TODO(EA2D): special case not needed with 2D EAs\u001b[39;00m\n\u001b[1;32m   1772\u001b[0m \u001b[39m# axis doesn't matter; we are really a single-dim object\u001b[39;00m\n\u001b[1;32m   1773\u001b[0m \u001b[39m# but are passed the axis depending on the calling routing\u001b[39;00m\n\u001b[1;32m   1774\u001b[0m \u001b[39m# if its REALLY axis 0, then this will be a reindex and not a take\u001b[39;00m\n\u001b[0;32m-> 1775\u001b[0m new_values \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mvalues\u001b[39m.\u001b[39;49mtake(indexer, fill_value\u001b[39m=\u001b[39;49mfill_value, allow_fill\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n\u001b[1;32m   1777\u001b[0m \u001b[39m# Called from three places in managers, all of which satisfy\u001b[39;00m\n\u001b[1;32m   1778\u001b[0m \u001b[39m#  this assertion\u001b[39;00m\n\u001b[1;32m   1779\u001b[0m \u001b[39massert\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mndim \u001b[39m==\u001b[39m \u001b[39m1\u001b[39m \u001b[39mand\u001b[39;00m new_mgr_locs \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.9/site-packages/pandas/core/arrays/sparse/array.py:1088\u001b[0m, in \u001b[0;36mSparseArray.take\u001b[0;34m(self, indices, allow_fill, fill_value)\u001b[0m\n\u001b[1;32m   1086\u001b[0m     dtype \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdtype\n\u001b[1;32m   1087\u001b[0m \u001b[39melif\u001b[39;00m allow_fill:\n\u001b[0;32m-> 1088\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_take_with_fill(indices, fill_value\u001b[39m=\u001b[39;49mfill_value)\n\u001b[1;32m   1089\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m   1090\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_take_without_fill(indices)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.9/site-packages/pandas/core/arrays/sparse/array.py:1122\u001b[0m, in \u001b[0;36mSparseArray._take_with_fill\u001b[0;34m(self, indices, fill_value)\u001b[0m\n\u001b[1;32m   1117\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mIndexError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mcannot do a non-empty take from an empty axes.\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m   1119\u001b[0m \u001b[39m# sp_indexer may be -1 for two reasons\u001b[39;00m\n\u001b[1;32m   1120\u001b[0m \u001b[39m# 1.) we took for an index of -1 (new)\u001b[39;00m\n\u001b[1;32m   1121\u001b[0m \u001b[39m# 2.) we took a value that was self.fill_value (old)\u001b[39;00m\n\u001b[0;32m-> 1122\u001b[0m sp_indexer \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msp_index\u001b[39m.\u001b[39;49mlookup_array(indices)\n\u001b[1;32m   1123\u001b[0m new_fill_indices \u001b[39m=\u001b[39m indices \u001b[39m==\u001b[39m \u001b[39m-\u001b[39m\u001b[39m1\u001b[39m\n\u001b[1;32m   1124\u001b[0m old_fill_indices \u001b[39m=\u001b[39m (sp_indexer \u001b[39m==\u001b[39m \u001b[39m-\u001b[39m\u001b[39m1\u001b[39m) \u001b[39m&\u001b[39m \u001b[39m~\u001b[39mnew_fill_indices\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "X = tfidf_lemmas.drop('issue', axis=1)  # Features (TF-IDF vectors)\n",
    "y = tfidf_lemmas['issue']  # Target variable (issue category)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an instance of the LinearSVC model\n",
    "model = LinearSVC()\n",
    "\n",
    "# Train the model\n",
    "model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate a classification report\n",
    "report = classification_report(y_test, y_pred)\n",
    "print(\"Classification Report:\")\n",
    "print(report)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Naive Bayes Using the Text Field"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Target feature = \"\"\n",
    "X = complaints[['complaint_narrative']]\n",
    "y = complaints['issue']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split( X, y, random_state = 321, stratify = y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Count Vectorizer - THIS IS NOT GONNA WORK.  Look at michael's notes and TOMO's code . . .  need a sparse matrix and all that jazz "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Fill in the code to fit and transform a CountVectorizer (using all defaults) on the text column of X_train and X_test\n",
    "vect = CountVectorizer()\n",
    "\n",
    "#Fit \n",
    "X_train_vec = vect.fit_transform(X_train[\"complaint_narrative\"])\n",
    "X_test_vec = vect.transform(X_test[\"complaint_narrative\"]) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "look at the vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vect.vocabulary_"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
