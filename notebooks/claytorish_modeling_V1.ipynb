{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import classification_report\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the entire .pkl file into a DataFrame\n",
    "complaints = pd.read_pickle('../data/complaints_nlp.pkl')\n",
    "\n",
    "# Select features\n",
    "columns_to_import = ['issue', 'tokens', 'stems', 'lemmas']\n",
    "complaints = complaints[columns_to_import]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bag of Words Analysis"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BOW: Tokens (Not Fruitful.  Focus on Lemmatised and Stemmed)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### BOW Tokens - Count Vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'CountVectorizer' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39m# Create an instance of CountVectorizer for tokens\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m vectorizer_tokens \u001b[39m=\u001b[39m CountVectorizer(min_df\u001b[39m=\u001b[39m\u001b[39m2\u001b[39m)\n\u001b[1;32m      4\u001b[0m X_tokens \u001b[39m=\u001b[39m vectorizer_tokens\u001b[39m.\u001b[39mfit_transform(complaints[\u001b[39m'\u001b[39m\u001b[39mtokens\u001b[39m\u001b[39m'\u001b[39m]\u001b[39m.\u001b[39mapply(\u001b[39mlambda\u001b[39;00m x: \u001b[39m'\u001b[39m\u001b[39m \u001b[39m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39mjoin(x)))\n\u001b[1;32m      6\u001b[0m \u001b[39m# Create a DataFrame from the sparse matrix\u001b[39;00m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'CountVectorizer' is not defined"
     ]
    }
   ],
   "source": [
    "# Create an instance of CountVectorizer for tokens\n",
    "\n",
    "vectorizer_tokens = CountVectorizer(min_df=2)\n",
    "X_tokens = vectorizer_tokens.fit_transform(complaints['tokens'].apply(lambda x: ' '.join(x)))\n",
    "\n",
    "# Create a DataFrame from the sparse matrix\n",
    "\n",
    "bag_of_words_tokens_df = pd.DataFrame.sparse.from_spmatrix(X_tokens, columns=vectorizer_tokens.get_feature_names_out())\n",
    "\n",
    "# Concatenate the bag-of-words DataFrame with the 'issue' column\n",
    "\n",
    "bag_of_words_tokens_df = pd.concat([complaints['issue'], bag_of_words_tokens_df], axis=1)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### BOW Tokens - Train/Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                     \r"
     ]
    }
   ],
   "source": [
    "X = bag_of_words_tokens_df.drop('issue', axis=1)\n",
    "y = bag_of_words_tokens_df['issue']\n",
    "X_train, X_test, y_train, y_test = tqdm(train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42), total=1, leave=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### BOW TOKENS - Naive Bayes Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chunderdamus/anaconda3/lib/python3.9/site-packages/sklearn/utils/validation.py:768: UserWarning: pandas.DataFrame with sparse columns found.It will be converted to a dense numpy array.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "y should be a 1d array, got an array of shape (282745, 2) instead.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[31], line 5\u001b[0m\n\u001b[1;32m      2\u001b[0m nb_model \u001b[39m=\u001b[39m MultinomialNB()\n\u001b[1;32m      4\u001b[0m \u001b[39m#Train The the model\u001b[39;00m\n\u001b[0;32m----> 5\u001b[0m nb_model\u001b[39m.\u001b[39;49mfit(X_train, y_train)\n\u001b[1;32m      7\u001b[0m \u001b[39m# Make Predictions from model\u001b[39;00m\n\u001b[1;32m      8\u001b[0m y_pred \u001b[39m=\u001b[39m nb_model\u001b[39m.\u001b[39mpredict(X_test)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.9/site-packages/sklearn/naive_bayes.py:749\u001b[0m, in \u001b[0;36m_BaseDiscreteNB.fit\u001b[0;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[1;32m    729\u001b[0m \u001b[39m\"\"\"Fit Naive Bayes classifier according to X, y.\u001b[39;00m\n\u001b[1;32m    730\u001b[0m \n\u001b[1;32m    731\u001b[0m \u001b[39mParameters\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    746\u001b[0m \u001b[39m    Returns the instance itself.\u001b[39;00m\n\u001b[1;32m    747\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    748\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_validate_params()\n\u001b[0;32m--> 749\u001b[0m X, y \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_check_X_y(X, y)\n\u001b[1;32m    750\u001b[0m _, n_features \u001b[39m=\u001b[39m X\u001b[39m.\u001b[39mshape\n\u001b[1;32m    752\u001b[0m labelbin \u001b[39m=\u001b[39m LabelBinarizer()\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.9/site-packages/sklearn/naive_bayes.py:583\u001b[0m, in \u001b[0;36m_BaseDiscreteNB._check_X_y\u001b[0;34m(self, X, y, reset)\u001b[0m\n\u001b[1;32m    581\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_check_X_y\u001b[39m(\u001b[39mself\u001b[39m, X, y, reset\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m):\n\u001b[1;32m    582\u001b[0m     \u001b[39m\"\"\"Validate X and y in fit methods.\"\"\"\u001b[39;00m\n\u001b[0;32m--> 583\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_validate_data(X, y, accept_sparse\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mcsr\u001b[39;49m\u001b[39m\"\u001b[39;49m, reset\u001b[39m=\u001b[39;49mreset)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.9/site-packages/sklearn/base.py:584\u001b[0m, in \u001b[0;36mBaseEstimator._validate_data\u001b[0;34m(self, X, y, reset, validate_separately, **check_params)\u001b[0m\n\u001b[1;32m    582\u001b[0m         y \u001b[39m=\u001b[39m check_array(y, input_name\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39my\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mcheck_y_params)\n\u001b[1;32m    583\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 584\u001b[0m         X, y \u001b[39m=\u001b[39m check_X_y(X, y, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mcheck_params)\n\u001b[1;32m    585\u001b[0m     out \u001b[39m=\u001b[39m X, y\n\u001b[1;32m    587\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m no_val_X \u001b[39mand\u001b[39;00m check_params\u001b[39m.\u001b[39mget(\u001b[39m\"\u001b[39m\u001b[39mensure_2d\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mTrue\u001b[39;00m):\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.9/site-packages/sklearn/utils/validation.py:1122\u001b[0m, in \u001b[0;36mcheck_X_y\u001b[0;34m(X, y, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, multi_output, ensure_min_samples, ensure_min_features, y_numeric, estimator)\u001b[0m\n\u001b[1;32m   1102\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m   1103\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00mestimator_name\u001b[39m}\u001b[39;00m\u001b[39m requires y to be passed, but the target y is None\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   1104\u001b[0m     )\n\u001b[1;32m   1106\u001b[0m X \u001b[39m=\u001b[39m check_array(\n\u001b[1;32m   1107\u001b[0m     X,\n\u001b[1;32m   1108\u001b[0m     accept_sparse\u001b[39m=\u001b[39maccept_sparse,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1119\u001b[0m     input_name\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mX\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m   1120\u001b[0m )\n\u001b[0;32m-> 1122\u001b[0m y \u001b[39m=\u001b[39m _check_y(y, multi_output\u001b[39m=\u001b[39;49mmulti_output, y_numeric\u001b[39m=\u001b[39;49my_numeric, estimator\u001b[39m=\u001b[39;49mestimator)\n\u001b[1;32m   1124\u001b[0m check_consistent_length(X, y)\n\u001b[1;32m   1126\u001b[0m \u001b[39mreturn\u001b[39;00m X, y\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.9/site-packages/sklearn/utils/validation.py:1143\u001b[0m, in \u001b[0;36m_check_y\u001b[0;34m(y, multi_output, y_numeric, estimator)\u001b[0m\n\u001b[1;32m   1141\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m   1142\u001b[0m     estimator_name \u001b[39m=\u001b[39m _check_estimator_name(estimator)\n\u001b[0;32m-> 1143\u001b[0m     y \u001b[39m=\u001b[39m column_or_1d(y, warn\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n\u001b[1;32m   1144\u001b[0m     _assert_all_finite(y, input_name\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39my\u001b[39m\u001b[39m\"\u001b[39m, estimator_name\u001b[39m=\u001b[39mestimator_name)\n\u001b[1;32m   1145\u001b[0m     _ensure_no_complex_data(y)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.9/site-packages/sklearn/utils/validation.py:1202\u001b[0m, in \u001b[0;36mcolumn_or_1d\u001b[0;34m(y, dtype, warn)\u001b[0m\n\u001b[1;32m   1193\u001b[0m         warnings\u001b[39m.\u001b[39mwarn(\n\u001b[1;32m   1194\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mA column-vector y was passed when a 1d array was\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   1195\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39m expected. Please change the shape of y to \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1198\u001b[0m             stacklevel\u001b[39m=\u001b[39m\u001b[39m2\u001b[39m,\n\u001b[1;32m   1199\u001b[0m         )\n\u001b[1;32m   1200\u001b[0m     \u001b[39mreturn\u001b[39;00m _asarray_with_order(xp\u001b[39m.\u001b[39mreshape(y, \u001b[39m-\u001b[39m\u001b[39m1\u001b[39m), order\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mC\u001b[39m\u001b[39m\"\u001b[39m, xp\u001b[39m=\u001b[39mxp)\n\u001b[0;32m-> 1202\u001b[0m \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m   1203\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39my should be a 1d array, got an array of shape \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m instead.\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(shape)\n\u001b[1;32m   1204\u001b[0m )\n",
      "\u001b[0;31mValueError\u001b[0m: y should be a 1d array, got an array of shape (282745, 2) instead."
     ]
    }
   ],
   "source": [
    "# Create an instance of the Naive Bayes model\n",
    "nb_model = MultinomialNB()\n",
    "\n",
    "#Train The the model\n",
    "nb_model.fit(X_train, y_train)\n",
    "\n",
    "# Make Predictions from model\n",
    "y_pred = nb_model.predict(X_test)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BOW Stems"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bag-of-Words Stems:\n",
      "   issue  aa  aaa  aaaf  aabout  aac  aacbna  aaccord  aaccount  aacord  ...  \\\n",
      "0      4   0    0     0       0    0       0        0         0       0  ...   \n",
      "1      3   0    0     0       0    0       0        0         0       0  ...   \n",
      "2      4   0    0     0       0    0       0        0         0       0  ...   \n",
      "3      1   0    0     0       0    0       0        0         0       0  ...   \n",
      "4      4   0    0     0       0    0       0        0         0       0  ...   \n",
      "\n",
      "   zion  zip  zipcod  zlimen  zombi  zone  zoom  zt  zuntafi  zwicker  \n",
      "0     0    0       0       0      0     0     0   0        0        0  \n",
      "1     0    0       0       0      0     0     0   0        0        0  \n",
      "2     0    0       0       0      0     0     0   0        0        0  \n",
      "3     0    0       0       0      0     0     0   0        0        0  \n",
      "4     0    0       0       0      0     0     0   0        0        0  \n",
      "\n",
      "[5 rows x 36811 columns]\n"
     ]
    }
   ],
   "source": [
    "# Create an instance of CountVectorizer for stems\n",
    "vectorizer_stems = CountVectorizer(min_df=2)\n",
    "X_stems = vectorizer_stems.fit_transform(complaints['stems'].apply(lambda x: ' '.join(x)))\n",
    "\n",
    "# Create a DataFrame from the sparse matrix\n",
    "bag_of_words_stems_df = pd.DataFrame.sparse.from_spmatrix(X_stems, columns=vectorizer_stems.get_feature_names_out())\n",
    "\n",
    "# Concatenate the bag-of-words DataFrame with the 'issue' column\n",
    "bag_of_words_stems_df = pd.concat([complaints['issue'], bag_of_words_stems_df], axis=1)\n",
    "\n",
    "print(\"Bag-of-Words Stems:\")\n",
    "print(bag_of_words_stems_df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BOW Lemmas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bag-of-Words Lemmas:\n",
      "   issue  aa  aaa  aaaf  aabout  aac  aacbna  aaccording  aaccount  aacording  \\\n",
      "0      4   0    0     0       0    0       0           0         0          0   \n",
      "1      3   0    0     0       0    0       0           0         0          0   \n",
      "2      4   0    0     0       0    0       0           0         0          0   \n",
      "3      1   0    0     0       0    0       0           0         0          0   \n",
      "4      4   0    0     0       0    0       0           0         0          0   \n",
      "\n",
      "   ...  zombie  zone  zoned  zoning  zoom  zoomed  zooming  zt  zuntafi  \\\n",
      "0  ...       0     0      0       0     0       0        0   0        0   \n",
      "1  ...       0     0      0       0     0       0        0   0        0   \n",
      "2  ...       0     0      0       0     0       0        0   0        0   \n",
      "3  ...       0     0      0       0     0       0        0   0        0   \n",
      "4  ...       0     0      0       0     0       0        0   0        0   \n",
      "\n",
      "   zwicker  \n",
      "0        0  \n",
      "1        0  \n",
      "2        0  \n",
      "3        0  \n",
      "4        0  \n",
      "\n",
      "[5 rows x 47258 columns]\n"
     ]
    }
   ],
   "source": [
    "# Create an instance of CountVectorizer for lemmas\n",
    "vectorizer_lemmas = CountVectorizer(min_df=2)\n",
    "X_lemmas = vectorizer_lemmas.fit_transform(complaints['lemmas'].apply(lambda x: ' '.join(x)))\n",
    "\n",
    "# Create a DataFrame from the sparse matrix\n",
    "bag_of_words_lemmas_df = pd.DataFrame.sparse.from_spmatrix(X_lemmas, columns=vectorizer_lemmas.get_feature_names_out())\n",
    "\n",
    "# Concatenate the bag-of-words DataFrame with the 'issue' column\n",
    "bag_of_words_lemmas_df = pd.concat([complaints['issue'], bag_of_words_lemmas_df], axis=1)\n",
    "\n",
    "print(\"Bag-of-Words Lemmas:\")\n",
    "print(bag_of_words_lemmas_df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Naive Bayes Using the Text Field"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Target feature = \"\"\n",
    "X = complaints[['complaint_narrative']]\n",
    "y = complaints['issue']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split( X, y, random_state = 321, stratify = y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Count Vectorizer - THIS IS NOT GONNA WORK.  Look at michael's notes and TOMO's code . . .  need a sparse matrix and all that jazz "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Fill in the code to fit and transform a CountVectorizer (using all defaults) on the text column of X_train and X_test\n",
    "vect = CountVectorizer()\n",
    "\n",
    "#Fit \n",
    "X_train_vec = vect.fit_transform(X_train[\"complaint_narrative\"])\n",
    "X_test_vec = vect.transform(X_test[\"complaint_narrative\"]) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "look at the vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vect.vocabulary_"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
