{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the entire .pkl file into a DataFrame\n",
    "complaints = pd.read_pickle('../data/complaints_nlp.pkl')\n",
    "\n",
    "# Select only the desired columns\n",
    "columns_to_import = ['issue', 'tokens', 'stems', 'lemmas']\n",
    "complaints = complaints[columns_to_import]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bag of Words Analysis"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BOW Tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bag-of-Words Tokens:\n",
      "   issue  aa  aaa  aaaf  aabout  aac  aacbna  aaccording  aaccount  aacording  \\\n",
      "0      4   0    0     0       0    0       0           0         0          0   \n",
      "1      3   0    0     0       0    0       0           0         0          0   \n",
      "2      4   0    0     0       0    0       0           0         0          0   \n",
      "3      1   0    0     0       0    0       0           0         0          0   \n",
      "4      4   0    0     0       0    0       0           0         0          0   \n",
      "\n",
      "   ...  zone  zoned  zones  zoning  zoom  zoomed  zooming  zt  zuntafi  \\\n",
      "0  ...     0      0      0       0     0       0        0   0        0   \n",
      "1  ...     0      0      0       0     0       0        0   0        0   \n",
      "2  ...     0      0      0       0     0       0        0   0        0   \n",
      "3  ...     0      0      0       0     0       0        0   0        0   \n",
      "4  ...     0      0      0       0     0       0        0   0        0   \n",
      "\n",
      "   zwicker  \n",
      "0        0  \n",
      "1        0  \n",
      "2        0  \n",
      "3        0  \n",
      "4        0  \n",
      "\n",
      "[5 rows x 50897 columns]\n"
     ]
    }
   ],
   "source": [
    "# Create an instance of CountVectorizer for tokens\n",
    "vectorizer_tokens = CountVectorizer(min_df=2)\n",
    "X_tokens = vectorizer_tokens.fit_transform(complaints['tokens'].apply(lambda x: ' '.join(x)))\n",
    "\n",
    "# Create a DataFrame from the sparse matrix\n",
    "bag_of_words_tokens_df = pd.DataFrame.sparse.from_spmatrix(X_tokens, columns=vectorizer_tokens.get_feature_names_out())\n",
    "\n",
    "# Concatenate the bag-of-words DataFrame with the 'issue' column\n",
    "bag_of_words_tokens_df = pd.concat([complaints['issue'], bag_of_words_tokens_df], axis=1)\n",
    "\n",
    "print(\"Bag-of-Words Tokens:\")\n",
    "print(bag_of_words_tokens_df.head())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BOW Stems"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bag-of-Words Stems:\n",
      "   issue  aa  aaa  aaaf  aabout  aac  aacbna  aaccord  aaccount  aacord  ...  \\\n",
      "0      4   0    0     0       0    0       0        0         0       0  ...   \n",
      "1      3   0    0     0       0    0       0        0         0       0  ...   \n",
      "2      4   0    0     0       0    0       0        0         0       0  ...   \n",
      "3      1   0    0     0       0    0       0        0         0       0  ...   \n",
      "4      4   0    0     0       0    0       0        0         0       0  ...   \n",
      "\n",
      "   zion  zip  zipcod  zlimen  zombi  zone  zoom  zt  zuntafi  zwicker  \n",
      "0     0    0       0       0      0     0     0   0        0        0  \n",
      "1     0    0       0       0      0     0     0   0        0        0  \n",
      "2     0    0       0       0      0     0     0   0        0        0  \n",
      "3     0    0       0       0      0     0     0   0        0        0  \n",
      "4     0    0       0       0      0     0     0   0        0        0  \n",
      "\n",
      "[5 rows x 36811 columns]\n"
     ]
    }
   ],
   "source": [
    "# Create an instance of CountVectorizer for stems\n",
    "vectorizer_stems = CountVectorizer(min_df=2)\n",
    "X_stems = vectorizer_stems.fit_transform(complaints['stems'].apply(lambda x: ' '.join(x)))\n",
    "\n",
    "# Create a DataFrame from the sparse matrix\n",
    "bag_of_words_stems_df = pd.DataFrame.sparse.from_spmatrix(X_stems, columns=vectorizer_stems.get_feature_names_out())\n",
    "\n",
    "# Concatenate the bag-of-words DataFrame with the 'issue' column\n",
    "bag_of_words_stems_df = pd.concat([complaints['issue'], bag_of_words_stems_df], axis=1)\n",
    "\n",
    "print(\"Bag-of-Words Stems:\")\n",
    "print(bag_of_words_stems_df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BOW Lemmas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bag-of-Words Lemmas:\n",
      "   issue  aa  aaa  aaaf  aabout  aac  aacbna  aaccording  aaccount  aacording  \\\n",
      "0      4   0    0     0       0    0       0           0         0          0   \n",
      "1      3   0    0     0       0    0       0           0         0          0   \n",
      "2      4   0    0     0       0    0       0           0         0          0   \n",
      "3      1   0    0     0       0    0       0           0         0          0   \n",
      "4      4   0    0     0       0    0       0           0         0          0   \n",
      "\n",
      "   ...  zombie  zone  zoned  zoning  zoom  zoomed  zooming  zt  zuntafi  \\\n",
      "0  ...       0     0      0       0     0       0        0   0        0   \n",
      "1  ...       0     0      0       0     0       0        0   0        0   \n",
      "2  ...       0     0      0       0     0       0        0   0        0   \n",
      "3  ...       0     0      0       0     0       0        0   0        0   \n",
      "4  ...       0     0      0       0     0       0        0   0        0   \n",
      "\n",
      "   zwicker  \n",
      "0        0  \n",
      "1        0  \n",
      "2        0  \n",
      "3        0  \n",
      "4        0  \n",
      "\n",
      "[5 rows x 47258 columns]\n"
     ]
    }
   ],
   "source": [
    "# Create an instance of CountVectorizer for lemmas\n",
    "vectorizer_lemmas = CountVectorizer(min_df=2)\n",
    "X_lemmas = vectorizer_lemmas.fit_transform(complaints['lemmas'].apply(lambda x: ' '.join(x)))\n",
    "\n",
    "# Create a DataFrame from the sparse matrix\n",
    "bag_of_words_lemmas_df = pd.DataFrame.sparse.from_spmatrix(X_lemmas, columns=vectorizer_lemmas.get_feature_names_out())\n",
    "\n",
    "# Concatenate the bag-of-words DataFrame with the 'issue' column\n",
    "bag_of_words_lemmas_df = pd.concat([complaints['issue'], bag_of_words_lemmas_df], axis=1)\n",
    "\n",
    "print(\"Bag-of-Words Lemmas:\")\n",
    "print(bag_of_words_lemmas_df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Naive Bayes Using the Text Field"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Target feature = \"\"\n",
    "X = complaints[['complaint_narrative']]\n",
    "y = complaints['issue']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split( X, y, random_state = 321, stratify = y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Count Vectorizer - THIS IS NOT GONNA WORK.  Look at michael's notes and TOMO's code . . .  need a sparse matrix and all that jazz "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Fill in the code to fit and transform a CountVectorizer (using all defaults) on the text column of X_train and X_test\n",
    "vect = CountVectorizer()\n",
    "\n",
    "#Fit \n",
    "X_train_vec = vect.fit_transform(X_train[\"complaint_narrative\"])\n",
    "X_test_vec = vect.transform(X_test[\"complaint_narrative\"]) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "look at the vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vect.vocabulary_"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
